{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "tweet.ipynb",
      "provenance": [],
      "mount_file_id": "1CCA2b0YgVm1zSogzktV81Pz16AivNOOp",
      "authorship_tag": "ABX9TyMxnCt9e3Bql3iFMkBvprx1",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/lawan94/KaggleCompetition/blob/main/tweet.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XCs4AQFPUgxg"
      },
      "source": [
        "#1.libraries"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UerMBc5IUjYm",
        "outputId": "ca134422-bd96-4246-8328-ab0c4a10d6c4"
      },
      "source": [
        "pip install tweet-preprocessor\n"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: tweet-preprocessor in /usr/local/lib/python3.7/dist-packages (0.6.0)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d5BSntRKi4gi",
        "outputId": "427aca25-1b81-400e-b40d-453ee9e23ff2"
      },
      "source": [
        "pip install contractions"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Installing collected packages: pyahocorasick, anyascii, textsearch, contractions\n",
            "Successfully installed anyascii-0.2.0 contractions-0.0.48 pyahocorasick-1.4.2 textsearch-0.0.21\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v3Ss5XIegZXM",
        "outputId": "790af675-f727-4c19-e0a6-9242babae896"
      },
      "source": [
        "#Manipulate data\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import math\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "#text preprocessing\n",
        "\n",
        "import preprocessor as p\n",
        "import spacy\n",
        "import gensim\n",
        "import nltk\n",
        "import string\n",
        "nltk.download('stopwords')\n",
        "from nltk.corpus import stopwords\n",
        "nltk.download(\"punkt\")\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
        "nltk.download('wordnet')\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "import re, string, unicodedata\n",
        "from nltk.tokenize import word_tokenize\n",
        "import contractions\n",
        "from collections import Counter\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow import keras\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from keras.preprocessing.text import Tokenizer,  text_to_word_sequence\n",
        "\n",
        "#classifiers\n",
        "from keras.utils import to_categorical\n",
        "import tensorflow.keras as k\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
        "from sklearn.naive_bayes import MultinomialNB"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zos6iFGVfHU1"
      },
      "source": [
        "trainTweet = pd.read_csv(\"/content/drive/MyDrive/KaggleCompet/NLP/nlp-getting-started/train.csv\")\n",
        "testTweet = pd.read_csv(\"/content/drive/MyDrive/KaggleCompet/NLP/nlp-getting-started/test.csv\")"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zOpJ-abwhI7t"
      },
      "source": [
        "#2. Overwiew of the data set"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qIIu-AMLfnZJ",
        "outputId": "ae93783c-1b75-414b-e3d6-5b520031944c"
      },
      "source": [
        "print(trainTweet.head()) #print first sows\n",
        "# 2 features are important for us : text and target\n"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "   id keyword  ...                                               text target\n",
            "0   1     NaN  ...  Our Deeds are the Reason of this #earthquake M...      1\n",
            "1   4     NaN  ...             Forest fire near La Ronge Sask. Canada      1\n",
            "2   5     NaN  ...  All residents asked to 'shelter in place' are ...      1\n",
            "3   6     NaN  ...  13,000 people receive #wildfires evacuation or...      1\n",
            "4   7     NaN  ...  Just got sent this photo from Ruby #Alaska as ...      1\n",
            "\n",
            "[5 rows x 5 columns]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OKWs_2jwgEZw",
        "outputId": "ece98230-1613-4415-e07d-5451f9acc835"
      },
      "source": [
        "print(trainTweet.value_counts())\n"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "id     keyword     location                        text                                                                                                                   target\n",
            "10833  wrecked     Lincoln                         @engineshed Great atmosphere at the British Lion gig tonight. Hearing is wrecked. http://t.co/oMNBAtJEAO               0         1\n",
            "3598   desolation  Stockholm, Sweden               ? This Weekend: Stockholm Sweden - Aug 8 at Copperfields http://t.co/6un7xC9Sve                                        1         1\n",
            "3583   desolate    Michigan, USA                   Psalm34:22 The Lord redeemeth the soul of his servants: and none of them that trust in him shall be desolate.          0         1\n",
            "3584   desolate    Boulder                         @joshacagan Your only option now is to move to an desolate island with nothing but a stack of DVDs you canÛªt watch.  0         1\n",
            "3587   desolate    Temporary Towers                @fotofill It looks so desolate. End of the world stuff. Gorgeous.                                                      0         1\n",
            "                                                                                                                                                                                   ..\n",
            "7169   mudslide    the burrow                      DORETTE THATS THE NAME OF THE MUDSLIDE CAKE MAKER                                                                      0         1\n",
            "7173   mudslide    plymouth                        @brobread looks like mudslide????                                                                                      1         1\n",
            "7174   mudslide    The Pumpkin Carriage of Dreams  @Lolly_Knickers It's a mudslide. \\nIt's like chewing on a rubber tyre.\\nAnd with those I'm DONE.\\n#vaginaorcake #GBBO  1         1\n",
            "7176   mudslide    London                          First impressions: glad hat man is leaving in lieu of more interesting ladies. Hope mudslide lady triumphs next week.  0         1\n",
            "48     ablaze      Birmingham                      @bbcmtd Wholesale Markets ablaze http://t.co/lHYXEOHY6C                                                                1         1\n",
            "Length: 5080, dtype: int64\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IL-rB6RhhN4T"
      },
      "source": [
        "# 3.Preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9rEpSGDLBrUg",
        "outputId": "00af206c-92ba-4a31-d6ad-c5f1d9ab49de"
      },
      "source": [
        "\n",
        "print(\"text data before cleaning \\n\")\n",
        "print(trainTweet.text)\n",
        "print(\"\\n\" + \"\\n\")\n",
        "print(\"after cleaning , it's look like that\" + \"\\n\")\n",
        "\n",
        "#getting all the texts \n",
        "texts = trainTweet.text\n",
        "cleaned_texts = texts.apply(p.clean)\n",
        "cleaned_texts = cleaned_texts.str.lower().str.replace('[^\\w\\s]',' ').str.replace('\\s\\s+', ' ')\n",
        "\n",
        "print(cleaned_texts)\n"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "text data before cleaning \n",
            "\n",
            "0       Our Deeds are the Reason of this #earthquake M...\n",
            "1                  Forest fire near La Ronge Sask. Canada\n",
            "2       All residents asked to 'shelter in place' are ...\n",
            "3       13,000 people receive #wildfires evacuation or...\n",
            "4       Just got sent this photo from Ruby #Alaska as ...\n",
            "                              ...                        \n",
            "7608    Two giant cranes holding a bridge collapse int...\n",
            "7609    @aria_ahrary @TheTawniest The out of control w...\n",
            "7610    M1.94 [01:04 UTC]?5km S of Volcano Hawaii. htt...\n",
            "7611    Police investigating after an e-bike collided ...\n",
            "7612    The Latest: More Homes Razed by Northern Calif...\n",
            "Name: text, Length: 7613, dtype: object\n",
            "\n",
            "\n",
            "\n",
            "after cleaning , it's look like that\n",
            "\n",
            "0       our deeds are the reason of this may allah for...\n",
            "1                   forest fire near la ronge sask canada\n",
            "2       all residents asked to shelter in place are be...\n",
            "3          people receive evacuation orders in california\n",
            "4       just got sent this photo from ruby as smoke fr...\n",
            "                              ...                        \n",
            "7608    two giant cranes holding a bridge collapse int...\n",
            "7609    the out of control wild fires in california ev...\n",
            "7610             m1 94 01 04 utc 5km s of volcano hawaii \n",
            "7611    police investigating after an e bike collided ...\n",
            "7612    the latest more homes razed by northern califo...\n",
            "Name: text, Length: 7613, dtype: object\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XaBN3VqzhCTB"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GuGyW5nAhTzU"
      },
      "source": [
        "# Collections of stopwords to be removed from the corpus"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HQZdi02txW55",
        "outputId": "4611a302-4d09-47f4-d8d6-4ba0431b2e64"
      },
      "source": [
        "sp = spacy.load('en_core_web_sm')\n",
        "allSTOPWORD = list()\n",
        "\n",
        "#spacy stopwords\n",
        "spacy_stopwords = list(sp.Defaults.stop_words) \n",
        "print(len(spacy_stopwords))\n",
        "\n",
        "#NLTK stopwords\n",
        "nltk_stopwords = list(stopwords.words(\"english\"))\n",
        "print(len(set(nltk_stopwords)))\n",
        "\n",
        "#Gensim stopwords\n",
        "gensim_stopwords = list(gensim.parsing.preprocessing.STOPWORDS)\n",
        "\n",
        "\n",
        "#try to stack them \n",
        "print(len(gensim_stopwords))\n",
        "\n",
        "allSTOPWORD.extend(gensim_stopwords)\n",
        "spacy_stopwords.extend(nltk_stopwords)\n",
        "\n",
        "allSTOPWORD.extend(spacy_stopwords)\n",
        "print(len(spacy_stopwords))\n",
        "#print(len(allSTOPWORD)) #we have 13926 stopwords\n",
        "# set to get unique stopwords\n",
        "allSTOPWORD = list(set(allSTOPWORD))\n",
        "print(len(allSTOPWORD)) #\n",
        "\n"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "326\n",
            "179\n",
            "337\n",
            "505\n",
            "413\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hTNhlJLJX1bF",
        "outputId": "b67c7e3f-faba-485b-da03-07a2cc554f7a"
      },
      "source": [
        "myStopWords1 = \"a able about above abst accordance according accordingly across\" \\\n",
        "\"act actually added adj affected affecting affects after afterwards \" \\\n",
        "\"again against ah all almost alone along already also although always \"\\\n",
        "\"am among amongst an and announce another any anybody anyhow anymore anyone\"\\\n",
        "\"anything anyway anyways anywhere apparently approximately are aren arent arise \"\\\n",
        "\"around as aside ask asking at auth available away awfully b back be became because \"\\\n",
        "\"become becomes becoming been before beforehand begin beginning beginnings begins behind \"\\\n",
        "\"being believe below beside besides between beyond biol both brief briefly but by c ca came\"\\\n",
        "\"can cannot can't cause causes certain certainly co com come comes contain containing contains \"\\\n",
        "\"could couldnt d date did didn't different do does doesn't doing done don't down downwards due \"\\\n",
        "\"during e each ed edu effect eg eight eighty either else elsewhere end ending enough especially \"\\\n",
        "\"et et-al etc even ever every everybody everyone everything everywhere ex except f far few ff fifth first\" \\\n",
        "\"five fix followed following follows for former formerly forth found four from further furthermore g gave get \"\\\n",
        "\"gets getting give given gives giving go goes gone got gotten h had happens hardly has hasn't have haven't having \"\\\n",
        "\"he hed hence her here hereafter hereby herein heres hereupon hers herself hes hi hid him himself his hither home \"\\\n",
        "\"how howbeit however hundred i id ie if i'll im immediate immediately importance important in inc indeed index \"\\\n",
        "\"information instead into invention inward is isn't it itd it'll its itself i've j just k keep\tkeeps kept kg km\" \\\n",
        "\"know known knows l largely last lately later latter latterly least less lest let lets like liked likely line little\" \\\n",
        "\"'ll look looking looks ltd m made mainly make makes many may maybe me mean means meantime meanwhile merely mg might \"\\\n",
        "\"million miss ml more moreover most mostly mr mrs much mug must my myself n na name namely nay nd near nearly necessarily \"\\\n",
        "\"necessary need needs neither never nevertheless new next nine ninety no nobody non none nonetheless noone nor normally nos \"\\\n",
        "\"not noted nothing now nowhere o obtain obtained obviously of off often oh ok okay old omitted on once one ones only onto or \"\\\n",
        "\"ord other others otherwise ought our ours ourselves out outside over overall owing own p page pages part particular particularly \"\\\n",
        "\"past per perhaps placed please plus poorly possible possibly potentially pp predominantly present previously primarily probably \"\\\n",
        "\"promptly proud provides put q que quickly quite qv r ran rather rd re readily really recent recently ref refs regarding \"\\\n",
        "\"regardless regards related relatively research respectively resulted resulting results right run s said same saw say saying says sec section\"\\\n",
        "\"see seeing seem seemed seeming seems seen self selves sent seven several shall she shed she'll shes should shouldn't \"\\\n",
        "\"show showed shown showns shows significant significantly similar similarly since six slightly so some somebody somehow someone \"\\\n",
        "\"somethan something sometime sometimes somewhat somewhere soon sorry specifically \"\\\n",
        "\"specified specify specifying still stop strongly sub substantially successfully such sufficiently suggest sup sure\"\\\n",
        "\n",
        "\n",
        "myStopWords2 = \"I a about an are as at be by com for from how  in is it of on or that the this to was what when where   who will with the www\"\\\n",
        "\"a's\table\tabout\tabove\taccording accordingly\tacross\tactually\tafter\tafterwards again\tagainst\tain't\tall\tallow allows\talmost\talone\"\\\n",
        "\"along\talready also\talthough\talways\tam\tamong amongst\tan\tand\tanother\tany anybody\tanyhow\tanyone\tanything\tanyway anyways\tanywhere\"\\\n",
        "\"apart\tappear\tappreciate appropriate\tare\taren't\taround\tas aside\task\tasking\tassociated\tat available\taway\tawfully\tbe\tbecame because\"\\\n",
        "\"become\tbecomes\tbecoming\tbeen before\tbeforehand\tbehind\tbeing\tbelieve below\tbeside\tbesides\tbest\tbetter between\tbeyond\tboth\tbrief\tbut\"\\\n",
        "\"by\tc'mon\tc's\tcame\tcan can't\tcannot\tcant\tcause\tcauses certain\tcertainly\tchanges\tclearly\tco com\tcome\tcomes\tconcerning\tconsequently \"\\\n",
        "\"consider\tconsidering\tcontain\tcontaining\tcontains corresponding\tcould\tcouldn't\tcourse\tcurrently definitely\tdescribed\tdespite\tdid\tdidn't \"\\\n",
        "\"different\tdo\tdoes\tdoesn't\tdoing don't\tdone\tdown\tdownwards\tduring each\tedu\teg\teight\teither else\telsewhere\tenough\tentirely\tespecially et\t\"\\\n",
        "\"etc\teven\tever\tevery everybody\teveryone\teverything\teverywhere\tex exactly\texample\texcept\tfar\tfew fifth\tfirst\tfive\tfollowed\tfollowing \"\\\n",
        "\"follows\tfor\tformer\tformerly\tforth four\tfrom\tfurther\tfurthermore\tget gets\tgetting\tgiven\tgives\tgo goes\tgoing\tgone\tgot\tgotten greetings\t\"\\\n",
        "\"had\thadn't\thappens\thardly has\thasn't\thave\thaven't\thaving he\the's\thello\thelp\thence her\there\there's\thereafter\thereby herein\thereupon\"\t\\\n",
        "\"hers\therself\thi him\thimself\this\thither\thopefully how\thowbeit\thowever\ti'd\ti'll i'm\ti've\tie\tif\tignored immediate\tin\tinasmuch\tinc\tindeed \"\\\n",
        "\"indicate\tindicated\tindicates\tinner\tinsofar instead\tinto\tinward\tis\tisn't it\tit'd\tit'll\tit's\tits itself\tjust\tkeep\tkeeps\tkept know\tknown\t\"\\\n",
        "\"knows\tlast\tlately later\tlatter\tlatterly\tleast\tless lest\tlet\tlet's\tlike\tliked likely\tlittle\tlook\tlooking\tlooks ltd\tmainly\tmany\tmay\tmaybe \"\\\n",
        "\"me\tmean\tmeanwhile\tmerely\tmight more\tmoreover\tmost\tmostly\tmuch must\tmy\tmyself\tname\tnamely nd\tnear\tnearly\tnecessary\tneed needs\tneither\t\"\\\n",
        "\"never\tnevertheless\tnew next\tnine\tno\tnobody\tnon none\tnoone\tnor\tnormally\tnot nothing\tnovel\tnow\tnowhere\tobviously of\toff\toften\toh\tok okay\t\"\\\n",
        "\n",
        "\"old\ton\tonce\tone ones\tonly\tonto\tor\tother others\totherwise\tought\tour\tours ourselves\tout\toutside\tover\toverall own\tparticular\tparticularly\"\t\\\n",
        "\"per\tperhaps placed\tplease\tplus\tpossible\tpresumably probably\tprovides\tque\tquite\tqv rather\trd\tre\treally\treasonably regarding\t\"\\\n",
        "\"regardless\tregards\trelatively\trespectively right\tsaid\tsame\tsaw\tsay saying\tsays\tsecond\tsecondly\tsee seeing\tseem\tseemed\"\t\\\n",
        "\"seeming\tseems seen\tself\tselves\tsensible\tsent serious\tseriously\tseven\tseveral\tshall she\tshould\tshouldn't\tsince\tsix so\t\"\\\n",
        "\"some\tsomebody\tsomehow\tsomeone something\tsometime\tsometimes\tsomewhat\tsomewhere soon\tsorry\tspecified\tspecify\tspecifying\" \\\n",
        "\"still\tsub\tsuch\tsup\tsure t's\ttake\ttaken\ttell\ttends th\tthan\tthank\tthanks\tthanx that\tthat's\tthats\tthe\ttheir theirs\t\"\\\n",
        "\"them\tthemselves\tthen\tthence there\tthere's\tthereafter\tthereby\ttherefore therein\ttheres\tthereupon\tthese\tthey they'd\tthey'll\t\"\\\n",
        "\"they're\tthey've\tthink third\tthis\tthorough\tthoroughly\tthose though\tthree\tthrough\tthroughout\tthru thus\tto\ttogether\ttoo\ttook \"\\\n",
        "\"toward\ttowards\ttried\ttries\ttruly try\ttrying\ttwice\ttwo\tun under\tunfortunately\tunless\tunlikely\tuntil unto\tup\tupon\tus\tuse used\t\"\\\n",
        "\"useful\tuses\tusing\tusually value\tvarious\tvery\tvia\tviz vs\twant\twants\twas\twasn't way\twe\twe'd\twe'll\twe're we've\twelcome\twell\twent\"\t\\\n",
        "\n",
        "\"which\twhile\twhither\twho who's\twhoever\twhole\twhom\twhose why\twill\twilling\twish\twith within\twithout\twon't\twonder\twould wouldn't\tyes\tyet\t\"\\\n",
        "\"whereby wherein\twhereupon\twherever whether\"\\\n",
        "\"you\tyou'd you'll\tyou're\tyou've\tyour\tyours yourself\tyourselves\tzero\" \"were weren't\twhat\twhat's\twhatever\twhen whence\twhenever where\twhere's\twhereafter whereas\"\n",
        "\t\n",
        "\n",
        "\n",
        "\n",
        "myStopWords = myStopWords1 +  \" \" + myStopWords2\n",
        "myStopWords = list(set(myStopWords.split()))\n",
        "print((len(myStopWords)))\n",
        "print(allSTOPWORD)\n",
        "\n",
        "allSTOPWORD.extend(myStopWords)\n",
        "STOP_WORDS = list(set(allSTOPWORD)) #keeping uniques stop words\n",
        "\n",
        "print(len(set(allSTOPWORD))) # 735 stopwords"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "566\n",
            "[\"'re\", 'nobody', 'nor', 'through', \"you're\", 'with', 'she', 'any', 'beforehand', 'thus', 'whereas', 'because', 'shan', 'own', 'why', 'an', 'too', 'themselves', 'weren', 'really', 'thereafter', \"that'll\", 'these', 'un', 'our', 'sincere', \"needn't\", 'again', 'may', 'everywhere', 'sixty', 'various', 'computer', 'above', 'bill', 'latter', 'me', 'further', 'therein', \"hasn't\", 'himself', 'hasnt', 'did', 'whom', 'move', 'below', 'make', 'seemed', 'thence', 'o', 'whether', 'anything', 'onto', \"wasn't\", 'needn', 'afterwards', 'km', \"'d\", 'or', 'doing', 'get', 'several', 'within', 'whereafter', 'almost', '‘re', 'already', 'yourselves', \"n't\", 'just', 'hadn', 'were', 'has', 'often', 'without', 'hasn', 'be', 's', 'both', 'what', 'describe', \"won't\", 'however', 'top', \"weren't\", 'n’t', 'ever', 'wouldn', 'than', 'whereupon', '‘ll', 'been', 'upon', 'fify', 'somehow', \"shouldn't\", 'fire', 'same', 'that', 'in', \"you'd\", 'anywhere', 'mill', '’d', 'mine', 'much', 'am', 'was', 'thick', 'become', '‘ve', 'its', 'herself', 'most', 'throughout', 'my', 'but', 'alone', 'even', 'up', 'and', 'mightn', 'first', 'sometime', 'during', 'm', 'the', 'their', 'anyway', 'used', 'mustn', 'll', '’s', 're', 'over', 'never', 'every', 'everyone', 'etc', 'else', 'won', 'y', 'besides', 'such', 'are', \"it's\", 'us', 'though', 'her', 'can', 'using', 'keep', 'seeming', 'didn', 'where', '’m', 'whoever', 'hundred', 'seems', 'being', 'nine', 'always', 'became', 'meanwhile', 've', 'a', 'ca', 'until', 'back', 'via', 'kg', 'whereby', 'sometimes', 'call', \"mustn't\", 'could', \"hadn't\", 'go', 'herein', 'will', 'six', 'side', 'perhaps', 'if', 'about', 'beside', \"'m\", 'his', 'it', 'give', 'd', 'should', 'isn', 'who', 'neither', 'becoming', 'since', \"'ve\", 'put', 'doesn', \"should've\", 'as', \"don't\", \"you've\", 'amongst', 'few', 'while', 'say', 'part', 'whither', 'thin', 'have', \"couldn't\", 'once', \"isn't\", 'ourselves', 'elsewhere', 'down', 'made', 'other', 'something', 'only', 'those', 'toward', 'behind', 'except', 'anyhow', 'eleven', 'due', 'cant', 'when', 'de', 'against', '‘s', 'thru', 'which', 'there', 'aren', 'do', 'therefore', 'around', 'don', 'not', 'namely', '’ll', 'whence', 'regarding', 'ltd', 'seem', 'three', 'beyond', 'anyone', 'five', 'thereupon', 'interest', 'least', 'along', 'moreover', 'hereupon', 'full', 'after', 'whose', 'i', 'would', 'empty', 'someone', 'somewhere', \"haven't\", 'less', 'former', 'before', 'very', 'cannot', 'towards', 'ie', 'shouldn', 'name', 'take', 'serious', 'hers', 'nothing', 'under', 'had', 'please', 'among', 'otherwise', '’ve', 'you', 'then', 'next', 'nowhere', 'on', 'of', 'others', 't', 'from', 'fifty', 'more', 'detail', 'him', 'couldnt', 'ours', 'yours', \"you'll\", 'indeed', 'twenty', 'whatever', 'eight', 'found', 'is', 'off', 'find', \"shan't\", 'must', 'none', 'them', '‘d', 'wherever', 'co', 'yet', 'might', 'four', 'fill', \"wouldn't\", 'couldn', 'hereafter', 'front', 'between', \"she's\", 'fifteen', 'yourself', 'together', 'system', 'unless', 'per', 'here', 'rather', '’re', \"mightn't\", 'latterly', 'two', 'so', 'this', 'they', 'ten', 'nevertheless', 'having', 'many', 'forty', 'amount', 'into', 'amoungst', 'done', 'all', \"didn't\", 'becomes', 'enough', 'wherein', 'no', \"'ll\", 'everything', 'twelve', 'for', 'noone', 'across', 'your', 'mostly', 'he', 'although', 'ma', 'how', 'at', '‘m', 'ain', 'haven', 'thereby', 'well', 'formerly', 'n‘t', 'hence', \"doesn't\", 'either', 'bottom', 'con', 'quite', 'myself', 'one', \"aren't\", 'does', 'still', 'whenever', 'hereby', 'inc', 'last', 'itself', 'theirs', 'whole', 'cry', 'out', 'see', 'wasn', 'some', 'another', 'each', 'we', 'also', 'now', 'by', \"'s\", 'third', 'eg', 'to', 'show']\n",
            "736\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mSnYkKAlmCxi"
      },
      "source": [
        "#Removing frequentWords\n",
        "\n",
        "Frequent words are common words of a particular domain.\n",
        "If we are working on any problem statement for a specific field, \n",
        "we can ignore common words in that domain because those frequent \n",
        "words don't give too much information.\n",
        "Implementation of frequent words removing\n",
        "Here we use the \"Counter\" function from the collection library to remove our corpus's frequent words."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WvJjUzkBl8zY",
        "outputId": "6112773c-e3df-4f67-a4a6-82b79b491157"
      },
      "source": [
        "## Implementation of frequent words removing\n",
        "def freq_words(text):\n",
        "\t\"\"\"\n",
        "\tReturn :- Most frequent words\n",
        "\tInput :- string\n",
        "\tOutput :-\n",
        "\t\"\"\"\n",
        "\t# tokenization\n",
        "\ttokens = word_tokenize(text)\n",
        "\tfor word in tokens:\n",
        "\t\tcounter[word]= +1\n",
        "\n",
        "\tFrequentWords = []\n",
        "\t# take top 10 frequent words\n",
        "\tfor (word, word_count) in counter.most_common(10):\n",
        "\t\tFrequentWords.append(word)\n",
        "\n",
        "\treturn FrequentWords\n",
        "\n",
        "\n",
        "# initiate object for counter\n",
        "counter = Counter()\n",
        "# some random text on machine learning\n",
        "ex_fw = cleaned_texts[0]\n",
        "# calling count_fw to calculate frequent words\n",
        "FrequentWords = freq_words(ex_fw)\n",
        "#print(f\"Top 10 Frequent Words from our example text :- \\n{FrequentWords}\")\n",
        "\n",
        "FrequentWords = cleaned_texts.apply(freq_words)\n",
        "FrequentWords\n"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0       [our, deeds, are, the, reason, of, this, may, ...\n",
              "1       [our, deeds, are, the, reason, of, this, may, ...\n",
              "2       [our, deeds, are, the, reason, of, this, may, ...\n",
              "3       [our, deeds, are, the, reason, of, this, may, ...\n",
              "4       [our, deeds, are, the, reason, of, this, may, ...\n",
              "                              ...                        \n",
              "7608    [our, deeds, are, the, reason, of, this, may, ...\n",
              "7609    [our, deeds, are, the, reason, of, this, may, ...\n",
              "7610    [our, deeds, are, the, reason, of, this, may, ...\n",
              "7611    [our, deeds, are, the, reason, of, this, may, ...\n",
              "7612    [our, deeds, are, the, reason, of, this, may, ...\n",
              "Name: text, Length: 7613, dtype: object"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qrroPt1Uq9Uy"
      },
      "source": [
        "#Removing of Rare words\n",
        "- Removing rare words text preprocessing technique is similar to eliminating\n",
        "- frequent words. We can remove more irregular words from the corpus."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1c1GzFCdq-0H"
      },
      "source": [
        "def rare_words(text):\n",
        "\t\"\"\"\n",
        "\tReturn :- Most Rare words\n",
        "\tInput :- string\n",
        "\tOutput :- list of rare words\n",
        "\t\"\"\"\n",
        "\t# tokenization\n",
        "\ttokens = word_tokenize(text)\n",
        "\tfor word in tokens:\n",
        "\t\tcounter[word]= +1\n",
        "\n",
        "\tRareWords = []\n",
        "\tnumber_rare_words = 10\n",
        "\t# take top 10 frequent words\n",
        "\tfrequentWords = counter.most_common()\n",
        "\tfor (word, word_count) in frequentWords[:-number_rare_words:-1]:\n",
        "\t\tRareWords.append(word)\n",
        "\n",
        "\treturn RareWords\n",
        "\n",
        "RareWords = cleaned_texts.apply(rare_words)"
      ],
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dlvXmXgejW32"
      },
      "source": [
        "#Implementation of removing of emoticons\n",
        "To remove emotions from the text, we need a list of emoticons; in this GitHub Repo, we can find all emoticons as a dictionary\n",
        "\n",
        "- use regular expressions to remove open and close\n",
        "- double brackets and anything in between them \n",
        "- we assume this is necessary based on our sample text.\n",
        "- replacing contractions with their expansions can be beneficial at this point, since \n",
        "- our word tokenizer will split words like \"didn't\" into \"did\" and \"n't.\n",
        "- It's not impossible to remedy this tokenization at a later stage, but \n",
        "- doing so prior makes it easier and more straightforward."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6Bxr6j_spJde"
      },
      "source": [
        "def remove_emojis(text):\n",
        "  text = re.sub(\"[\"u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
        "          u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
        "          u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
        "          u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
        "          u\"\\U00002500-\\U00002BEF\"  # chinese char\n",
        "          u\"\\U00002702-\\U000027B0\"\n",
        "          u\"\\U00002702-\\U000027B0\"\n",
        "          u\"\\U000024C2-\\U0001F251\"\n",
        "          u\"\\U0001f926-\\U0001f937\"\n",
        "          u\"\\U00010000-\\U0010ffff\"\n",
        "          u\"\\u2640-\\u2642\"\n",
        "          u\"\\u2600-\\u2B55\"\n",
        "          u\"\\u200d\"\n",
        "          u\"\\u23cf\"\n",
        "          u\"\\u23e9\"\n",
        "          u\"\\u231a\"\n",
        "          u\"\\ufe0f\"  # dingbats\n",
        "          u\"\\u3030\"\n",
        "          \"]+\", ' ', text) \n",
        "  text = re.sub(r'\\d+', '', text) #removing numbers\n",
        "  text = re.sub(r':.*$', \":\",text) # removing url  \n",
        "  text = re.sub('\\[[^]]*\\]', '', text) \n",
        "  text = contractions.fix(text) #\n",
        "  text = re.sub(r'\\s{4}', '', text)    \n",
        "  return text \n",
        "\n",
        "def filterStopWords(text):\n",
        "  tokens = word_tokenize(text) # create tokens\n",
        "  tokens = [word.lower() for word in tokens if word not in string.punctuation or word.isdigit()] # making words lowercase\n",
        "  filteredWords = [lemmatizer.lemmatize(token) for token in tokens if token  not in STOP_WORDS]\n",
        "  #FrequentWords = cleaned_texts.apply(freq_words)\n",
        "\n",
        "  #filteredWords = [token for token in filteredWords if token not in FrequentWords]\n",
        "  return \" \".join(filteredWords)\n",
        "\n",
        "\n"
      ],
      "execution_count": 55,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d7ClCGt3g_JY",
        "outputId": "4c69e459-1770-44a7-8bf3-ee105db0d0a6"
      },
      "source": [
        "#let’s remove the whitespace from the beginning of the text. The whitespace consists of four spaces.\n",
        "\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "\n",
        "# Implementation of removing punctuations using string library\n",
        "\n",
        "from string import punctuation\n",
        "\n",
        "\n",
        " \n",
        "def clean_and_remove_stopWords(text):\n",
        "  text = remove_emojis(text)\n",
        "  return filterStopWords(text)\n",
        " \n",
        " \n",
        "  \n",
        "\n",
        "corpus = trainTweet.text.apply(clean_and_remove_stopWords)\n",
        "corpus_test = testTweet.text.apply(clean_and_remove_stopWords)\n",
        "\n",
        "corpus"
      ],
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0                    deed reason earthquake allah forgive\n",
              "1                             forest la ronge sask canada\n",
              "2       resident asked 'shelter place notified officer...\n",
              "3       people receive wildfire evacuation order calif...\n",
              "4           photo ruby alaska smoke wildfire pours school\n",
              "                              ...                        \n",
              "7608    giant crane holding bridge collapse nearby hom...\n",
              "7609    aria_ahrary thetawniest control wild fire cali...\n",
              "7610                                                   m.\n",
              "7611    police investigating e-bike collided car portu...\n",
              "7612                                               latest\n",
              "Name: text, Length: 7613, dtype: object"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 56
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TIOa8u5YkJ2q"
      },
      "source": [
        "## Tokenisation\n",
        "- Transform each text in a list of tokens, converted into identifiers, using the NLTK library's `wordpunct_tokenize` function. \n",
        "- Use the `get` method of the` dictionary` variable to convert tokens into identifiers. \n",
        "- Words not found in the vocabularies must be replaced by the identification of the token `\" <PAD> \"`.\n",
        "\n",
        "- Visualize the distribution of the number of tokens by text with a histogram, thanks to the `hist` function of the Matplotlib library (` matplotlib.pyplot`)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 316
        },
        "id": "RM4U__IA0uwD",
        "outputId": "301efd34-cb5e-48ba-f5e0-1c6c1077afcd"
      },
      "source": [
        "length = [len(corpus[i]) for i in range(len(corpus))]\n",
        "plt.hist(length, bins=30)\n",
        "plt.show()\n",
        "print(np.max(length))\n",
        "\n",
        "count_vect = CountVectorizer(analyzer='word', token_pattern=r'\\w{1,}')\n",
        "count_vect.fit(corpus)\n",
        "\n",
        "# transform the training and validation data using count vectorizer object\n",
        "feature_1Transformed =  count_vect.transform(corpus)\n",
        "#xvalid_count =  count_vect.transform(valid_x)\n",
        "print(\"its look like a very sparse matrix\")\n",
        "feature_1Transformed.toarray().shape"
      ],
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        },
        {
          "output_type": "stream",
          "text": [
            "137\n",
            "its look like a very sparse matrix\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(7613, 13398)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 57
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nsKT8kgIkctp"
      },
      "source": [
        "#3.1 Count vectorizer\n",
        "\n",
        "Count Vector is a matrix notation of the dataset in which every row represents a document from the corpus, every column represents a term from the corpus, and every cell represents the frequency count of a particular term in a particular document"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GkjhMcXK1svZ",
        "outputId": "f1b82942-15cb-4438-907e-1f60926d013e"
      },
      "source": [
        "count_vect = CountVectorizer(analyzer='word', token_pattern=r'\\w{1,}', min_df=10)\n",
        "count_vect.fit(corpus)\n",
        "\n",
        "X_cv = count_vect.transform(corpus)\n",
        "Xtest_cv = count_vect.transform(corpus_test)\n",
        "\n",
        "y = trainTweet.target\n",
        "\n",
        "\n",
        "print(X_cv.shape, Xtest_cv.shape)"
      ],
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(7613, 1054) (3263, 1054)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M-ouH8xjkeRg"
      },
      "source": [
        "#3.2 Tf-idf\n",
        "\n",
        "TF-IDF score represents the relative importance of a term in the document and the entire corpus based on two terms: the first computes the normalized Term Frequency (TF), the second term is the Inverse Document Frequency (IDF), computed as the logarithm of the number of the documents in the corpus divided by the number of documents where the specific term appears.\n",
        "\n",
        "TF(t) = (Number of times term t appears in a document) / (Total number of terms in the document)\n",
        "IDF(t) = log_e(Total number of documents / Number of documents with term t in it)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_NvBVlEGHrp0"
      },
      "source": [
        "tfidf_vect = TfidfVectorizer(min_df=3)\n",
        "tfidf_vect.fit(corpus)\n",
        "\n",
        "X_tf = tfidf_vect.transform(corpus)\n"
      ],
      "execution_count": 59,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wn48lnTCtRjG"
      },
      "source": [
        "#3.3 Word embeddings\n",
        "A word embedding is a form of representing words and documents using a dense vector representation. The position of a word within the vector space is learned from text and is based on the words that surround the word when it is used. Word embeddings can be trained using the input corpus itself or can be generated using pre-trained word embeddings such as Glove, FastText, and Word2Vec"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fujmt905C5h_",
        "outputId": "f1d450b5-e08a-454f-8f96-fbac186af2b5"
      },
      "source": [
        "\n",
        "from keras.preprocessing.text import Tokenizer,  text_to_word_sequence\n",
        "\n",
        "NUM_WORDS=10000 # on garde seulement les 20 tokens les plus fréquents\n",
        "t = Tokenizer(num_words=NUM_WORDS,oov_token=\"<OOV>\")\n",
        "t.fit_on_texts(texts)\n",
        "sequences = t.texts_to_sequences(texts)\n",
        "\n",
        "word_index = t.word_index\n",
        "print('Found %s unique tokens.' % len(word_index))\n",
        "vocabulary_size=min(len(word_index)+1,NUM_WORDS)\n",
        "\n",
        "\n",
        "max_length = 452\n",
        "\n",
        "X = pad_sequences(sequences,\n",
        "                  maxlen=max_length,\n",
        "                  truncating=\"post\",\n",
        "                  padding=\"post\" \n",
        "            )"
      ],
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found 22701 unique tokens.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BF-d2nOS2nVh",
        "outputId": "92757b0b-c930-43c7-e762-f02b9bae9220"
      },
      "source": [
        "Xtrain,Xtest, ytrain, ytest = train_test_split(X_cv, y, test_size=.2)\n",
        "#Xtrain,Xtest, ytrain, ytest = train_test_split(X, y, test_size=.1)\n",
        "\n",
        "def mlModel(classifier, Xtrain,ytrain, Xtest, ytest):\n",
        "  clf = classifier\n",
        "  clf.fit(Xtrain, ytrain)\n",
        "  return clf.score(Xtest, ytest)\n",
        "\n",
        "mlModel(MultinomialNB(), Xtrain, ytrain, Xtest, ytest)"
      ],
      "execution_count": 61,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.7760998030203545"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 61
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FIKy9DH2L9Yd"
      },
      "source": [
        "## Convolutional neural network design\n",
        "\n",
        "Design the network architecture in sequential mode with the Keras API of the TensorFlow library (`tensorflow.keras`). The network must have the following layers:\n",
        "\n",
        "- Input layer (`layer.Input`)\n",
        "- Word representation layer\n",
        "- Parallel convolution layers (`layers.Conv1D`)\n",
        "  - Sub-sampling layer (`layers.GlobalMaxPooling1D`) after each convolution layer\n",
        "- Concatenation layer (\"layer.concatenate\")\n",
        "- Regularization layer by dropout (`layers.Dropout`)\n",
        "- Dense layer with softmax activation (`layers.Dense`)\n",
        "\n",
        "Configure the word representation layer to use pre-trained representations without updating them (`weights` and` trainable` arguments).\n",
        "\n",
        "Provide filters of width 2, 3, 4 and 5, with 50 filters per width."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1eXEyL_-L8jp"
      },
      "source": [
        "import tensorflow.keras as k\n",
        "\n",
        "filter_sizes = [2, 3]\n",
        "n_filters = 100\n",
        "\n",
        "# Couche d'entrée\n",
        "word_input = k.layers.Input(shape=(max_length, ))\n",
        "# Couche de représentation des mots\n",
        "embedding_layer = k.layers.Embedding(input_dim=(vocabulary_size),\n",
        "                                     output_dim=300,trainable=False)(word_input)\n",
        "# Couches de convolutions en parallèle\n",
        "pooled_outputs = []\n",
        "for size in filter_sizes:\n",
        "  convolution_layer = k.layers.Conv1D(n_filters, # nombre de filtres pour cette taille\n",
        "                                      kernel_size=size, # largeur du filtre\n",
        "                                      activation=\"tanh\")(embedding_layer)\n",
        "  # Couche de sous-échantillonnage\n",
        "  pooled_output = k.layers.GlobalMaxPooling1D()(convolution_layer)\n",
        "  pooled_outputs.append(pooled_output)\n",
        "# Concaténation des attributs calculés par convolution + sous-échantillonnage\n",
        "feature_map = k.layers.Concatenate()(pooled_outputs) # vecteur réel de taille n_filters * len(filter_sizes)\n",
        "# Couche de régularisation\n",
        "dropout_layer = k.layers.Dropout(0.5)(feature_map)\n",
        "# Couche dense pour la classification\n",
        "classification_layer = k.layers.Dense(units=2,\n",
        "                                      activation=\"softmax\")(dropout_layer)"
      ],
      "execution_count": 50,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2v4fyiNnMnCD",
        "outputId": "a69a09e8-d953-4f91-ce98-399d33ec6aad"
      },
      "source": [
        "# Instanciation du modèle\n",
        "\n",
        "#inputs -> wordinput\n",
        "#outputs -> classifLayer\n",
        "def compileModel(inputs, outputs, optimizer=\"RMSprop\"):\n",
        "  model = k.Model(inputs=inputs, outputs=outputs)\n",
        "  model.compile(loss=\"categorical_crossentropy\",optimizer=optimizer,metrics=[\"accuracy\"])\n",
        "  model.summary()\n",
        "  return model\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "#Splitting the data into test and validation data.\n",
        "\n",
        "def deepModel(modelName, X, y, ts1=0.5, ts2=0.2, batch_size=1, epochs=10):\n",
        "  Xtrain,x_temp,ytrain,y_temp= train_test_split(X,y,test_size= ts1)\n",
        "  Xtest,Xval,ytest,yval = train_test_split(Xtrain,ytrain, test_size = ts2)\n",
        "  modelName.fit(x=Xtrain, y=ytrain, validation_data=(Xval, yval), batch_size=batch_size, epochs=epochs)\n",
        "  print(\"prediction for the test fold\")\n",
        "  print(modelName.evaluate(Xtest, ytest))\n",
        "  return modelName, (Xtest, ytest)\n",
        "\n",
        "cnnModel = compileModel(word_input, classification_layer)\n",
        "cnnModel_, (Xtest, ytest) = deepModel(cnnModel, X,ydeep)"
      ],
      "execution_count": 63,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"model_1\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_10 (InputLayer)           [(None, 452)]        0                                            \n",
            "__________________________________________________________________________________________________\n",
            "embedding_7 (Embedding)         (None, 452, 300)     3000000     input_10[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "conv1d (Conv1D)                 (None, 451, 100)     60100       embedding_7[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "conv1d_1 (Conv1D)               (None, 450, 100)     90100       embedding_7[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "global_max_pooling1d (GlobalMax (None, 100)          0           conv1d[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "global_max_pooling1d_1 (GlobalM (None, 100)          0           conv1d_1[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "concatenate (Concatenate)       (None, 200)          0           global_max_pooling1d[0][0]       \n",
            "                                                                 global_max_pooling1d_1[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "dropout (Dropout)               (None, 200)          0           concatenate[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "dense (Dense)                   (None, 2)            402         dropout[0][0]                    \n",
            "==================================================================================================\n",
            "Total params: 3,150,602\n",
            "Trainable params: 150,602\n",
            "Non-trainable params: 3,000,000\n",
            "__________________________________________________________________________________________________\n",
            "Epoch 1/10\n",
            "3806/3806 [==============================] - 57s 15ms/step - loss: 0.6458 - accuracy: 0.6425 - val_loss: 0.4915 - val_accuracy: 0.7743\n",
            "Epoch 2/10\n",
            "3806/3806 [==============================] - 56s 15ms/step - loss: 0.5038 - accuracy: 0.7632 - val_loss: 0.3552 - val_accuracy: 0.8438\n",
            "Epoch 3/10\n",
            "3806/3806 [==============================] - 57s 15ms/step - loss: 0.3858 - accuracy: 0.8346 - val_loss: 0.2296 - val_accuracy: 0.9173\n",
            "Epoch 4/10\n",
            "3806/3806 [==============================] - 64s 17ms/step - loss: 0.2695 - accuracy: 0.8953 - val_loss: 0.1381 - val_accuracy: 0.9541\n",
            "Epoch 5/10\n",
            "3806/3806 [==============================] - 57s 15ms/step - loss: 0.2068 - accuracy: 0.9296 - val_loss: 0.1246 - val_accuracy: 0.9606\n",
            "Epoch 6/10\n",
            "3806/3806 [==============================] - 57s 15ms/step - loss: 0.1595 - accuracy: 0.9451 - val_loss: 0.0805 - val_accuracy: 0.9751\n",
            "Epoch 7/10\n",
            "3806/3806 [==============================] - 57s 15ms/step - loss: 0.1246 - accuracy: 0.9606 - val_loss: 0.0612 - val_accuracy: 0.9829\n",
            "Epoch 8/10\n",
            "3806/3806 [==============================] - 56s 15ms/step - loss: 0.1121 - accuracy: 0.9615 - val_loss: 0.0676 - val_accuracy: 0.9803\n",
            "Epoch 9/10\n",
            "3806/3806 [==============================] - 57s 15ms/step - loss: 0.1112 - accuracy: 0.9697 - val_loss: 0.0347 - val_accuracy: 0.9816\n",
            "Epoch 10/10\n",
            "3806/3806 [==============================] - 57s 15ms/step - loss: 0.1044 - accuracy: 0.9751 - val_loss: 0.0410 - val_accuracy: 0.9856\n",
            "prediction for the test fold\n",
            "96/96 [==============================] - 8s 77ms/step - loss: 0.0620 - accuracy: 0.9846\n",
            "[0.06201329454779625, 0.9845597743988037]\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}
